{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logica del Juego"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gym\n",
    "import random\n",
    "from termcolor import colored, cprint\n",
    "import time\n",
    "import math\n",
    "from IPython.display import clear_output\n",
    "from matplotlib import pyplot as plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 0 = empty_space; 1 = player; 2 = goal \n",
    "def draw_map(size, player_position, goal_position):\n",
    "    map = np.zeros((size[1], size[0]), dtype=np.int)\n",
    "    map[player_position[1], player_position[0]] = 1\n",
    "    map[goal_position[1], goal_position[0]] = 2\n",
    "    \n",
    "    return map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class GameEnviroment:\n",
    "    def __init__(self, map, size, player_initial_state, goal_position):\n",
    "        self.map = map\n",
    "        self.size = size\n",
    "        self.player_state = player_initial_state\n",
    "        self.goal = goal_position \n",
    "        self.rewards = {\n",
    "            \"walk\": -1,\n",
    "            \"fall\": -5,\n",
    "            \"goal\": 20\n",
    "        }\n",
    "\n",
    "    def __str__(self):\n",
    "        return str(self.map)\n",
    "    \n",
    "    def reset(self, initial_map, player_initial_state):\n",
    "        self.map = initial_map\n",
    "        self.player_state = player_initial_state\n",
    "        return ((10 * self.player_state[0]) + self.player_state[1])\n",
    "\n",
    "    def is_in_goal(self):\n",
    "        return (self.player_state[0] == self.goal[0] and self.player_state[1] == self.goal[1])\n",
    "    \n",
    "    def has_fallen_of_map(self):\n",
    "        x_range = np.array(range(self.size[0]))\n",
    "        y_range = np.array(range(self.size[1]))\n",
    "        return (self.player_state[1] not in y_range or self.player_state[0] not in x_range)\n",
    "\n",
    "    def move_player_left(self):\n",
    "        self.player_state[0] = self.player_state[0] - 1\n",
    "        if self.is_in_goal():\n",
    "            return (10*self.player_state[0] + self.player_state[1]), self.rewards[\"goal\"], True\n",
    "        if self.has_fallen_of_map():\n",
    "            return (10*self.player_state[0] + self.player_state[1]), self.rewards[\"fall\"], True\n",
    "        self.map = draw_map(self.size, self.player_state, self.goal)\n",
    "        return (10*self.player_state[0] + self.player_state[1]), self.rewards[\"walk\"], False\n",
    "    \n",
    "    def move_player_right(self):\n",
    "        self.player_state[0] = self.player_state[0] + 1\n",
    "        if self.is_in_goal():\n",
    "            return (10*self.player_state[0] + self.player_state[1]), self.rewards[\"goal\"], True\n",
    "        if self.has_fallen_of_map():\n",
    "            return (10*self.player_state[0] + self.player_state[1]), self.rewards[\"fall\"], True\n",
    "        self.map = draw_map(self.size, self.player_state, self.goal)\n",
    "        return (10*self.player_state[0] + self.player_state[1]), self.rewards[\"walk\"], False\n",
    "\n",
    "    def move_player_up(self):\n",
    "        self.player_state[1] = self.player_state[1] - 1\n",
    "        if self.is_in_goal():\n",
    "            return (10*self.player_state[0] + self.player_state[1]), self.rewards[\"goal\"], True\n",
    "        if self.has_fallen_of_map():\n",
    "            return (10*self.player_state[0] + self.player_state[1]), self.rewards[\"fall\"], True\n",
    "        self.map = draw_map(self.size, self.player_state, self.goal)\n",
    "        return (10*self.player_state[0] + self.player_state[1]), self.rewards[\"walk\"], False\n",
    "    \n",
    "    def move_player_down(self):\n",
    "        self.player_state[1] = self.player_state[1] + 1\n",
    "        if self.is_in_goal():\n",
    "            return (10*self.player_state[0] + self.player_state[1]), self.rewards[\"goal\"], True\n",
    "        if self.has_fallen_of_map():\n",
    "            return (10*self.player_state[0] + self.player_state[1]), self.rewards[\"fall\"], True\n",
    "        self.map = draw_map(self.size, self.player_state, self.goal)\n",
    "        return (10*self.player_state[0] + self.player_state[1]), self.rewards[\"walk\"], False\n",
    "        \n",
    "    # 0 = LEFT, 1 = RIGHT, 2 = UP, 3 = DOWN\n",
    "    def step(self, action):\n",
    "        if action == 0:\n",
    "            new_state, reward, done = self.move_player_left()\n",
    "        if action == 1:\n",
    "            new_state, reward, done = self.move_player_right()\n",
    "        if action == 2:\n",
    "            new_state, reward, done = self.move_player_up()\n",
    "        if action == 3:\n",
    "            new_state, reward, done = self.move_player_down()\n",
    "\n",
    "        # applying wind\n",
    "        PrA = 0.1\n",
    "        PrB = 0.2\n",
    "        PrC = 0.15\n",
    "        if self.player_state[0] == 3:\n",
    "            random_number = np.random.uniform(low=0.0, high=1.0)\n",
    "            if random_number <= PrA:\n",
    "                new_state, reward, done = self.move_player_up()\n",
    "        \n",
    "        if self.player_state[0] == 4:\n",
    "            random_number = np.random.uniform(low=0.0, high=1.0)\n",
    "            if random_number <= PrB:\n",
    "                new_state, reward, done = self.move_player_up()\n",
    "                new_state, reward, done = self.move_player_up()\n",
    "\n",
    "        if self.player_state[0] == 5:\n",
    "            random_number = np.random.uniform(low=0.0, high=1.0)\n",
    "            if random_number <= PrC:\n",
    "                new_state, reward, done = self.move_player_up()\n",
    "        \n",
    "        return new_state, reward, done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0 0 0 0 0 0 0 0]\n",
      " [0 1 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 2 0]\n",
      " [0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "map_size = [9, 7]\n",
    "player_initial_pos = [1, 1]\n",
    "goal_pos = [7, 4]\n",
    "\n",
    "initial_map = draw_map(map_size, player_initial_pos, goal_pos)\n",
    "env = GameEnviroment(initial_map, map_size, player_initial_pos, goal_pos)\n",
    "print(env)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inicializando parametros "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_episodes = 15000\n",
    "max_steps_per_episode = 100\n",
    "\n",
    "learning_rate = 0.2\n",
    "discount_rate = 0.95\n",
    "\n",
    "rewards_avg = []\n",
    "\n",
    "action_space_size = 4\n",
    "state_space_size = 200\n",
    "\n",
    "q_table = np.zeros((state_space_size, action_space_size))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Corriendo el algoritmo Q-Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "average  0\n",
      "state: 11, action: 1, new state: 21, reward: -1, done?: False\n",
      "state: 21, action: 1, new state: 31, reward: -1, done?: False\n",
      "state: 31, action: 1, new state: 41, reward: -1, done?: False\n",
      "state: 41, action: 3, new state: 42, reward: -1, done?: False\n",
      "state: 42, action: 0, new state: 32, reward: -1, done?: False\n",
      "state: 32, action: 3, new state: 32, reward: -1, done?: False\n",
      "state: 32, action: 3, new state: 33, reward: -1, done?: False\n",
      "state: 33, action: 0, new state: 23, reward: -1, done?: False\n",
      "state: 23, action: 3, new state: 24, reward: -1, done?: False\n",
      "state: 24, action: 2, new state: 23, reward: -1, done?: False\n",
      "state: 23, action: 0, new state: 13, reward: -1, done?: False\n",
      "state: 13, action: 2, new state: 12, reward: -1, done?: False\n",
      "state: 12, action: 3, new state: 13, reward: -1, done?: False\n",
      "state: 13, action: 1, new state: 23, reward: -1, done?: False\n",
      "state: 23, action: 3, new state: 24, reward: -1, done?: False\n",
      "state: 24, action: 3, new state: 25, reward: -1, done?: False\n",
      "state: 25, action: 2, new state: 24, reward: -1, done?: False\n",
      "state: 24, action: 1, new state: 34, reward: -1, done?: False\n",
      "state: 34, action: 0, new state: 24, reward: -1, done?: False\n",
      "state: 24, action: 3, new state: 25, reward: -1, done?: False\n",
      "state: 25, action: 0, new state: 15, reward: -1, done?: False\n",
      "state: 15, action: 2, new state: 14, reward: -1, done?: False\n",
      "state: 14, action: 2, new state: 13, reward: -1, done?: False\n",
      "state: 13, action: 3, new state: 14, reward: -1, done?: False\n",
      "state: 14, action: 0, new state: 4, reward: -1, done?: False\n",
      "state: 4, action: 1, new state: 14, reward: -1, done?: False\n",
      "state: 14, action: 3, new state: 15, reward: -1, done?: False\n",
      "state: 15, action: 3, new state: 16, reward: -1, done?: False\n",
      "state: 16, action: 2, new state: 15, reward: -1, done?: False\n",
      "state: 15, action: 0, new state: 5, reward: -1, done?: False\n",
      "state: 5, action: 0, new state: -5, reward: -5, done?: True\n",
      "state: -5, action: 0, new state: -15, reward: -5, done?: True\n",
      "state: -15, action: 3, new state: -14, reward: -5, done?: True\n",
      "state: -14, action: 0, new state: -24, reward: -5, done?: True\n",
      "state: -24, action: 1, new state: -14, reward: -5, done?: True\n",
      "state: -14, action: 2, new state: -15, reward: -5, done?: True\n",
      "state: -15, action: 1, new state: -5, reward: -5, done?: True\n",
      "state: -5, action: 2, new state: -6, reward: -5, done?: True\n",
      "state: -6, action: 1, new state: 4, reward: -1, done?: False\n",
      "state: 4, action: 2, new state: 3, reward: -1, done?: False\n",
      "state: 3, action: 0, new state: -7, reward: -5, done?: True\n",
      "state: -7, action: 0, new state: -17, reward: -5, done?: True\n",
      "state: -17, action: 2, new state: -18, reward: -5, done?: True\n",
      "state: -18, action: 2, new state: -19, reward: -5, done?: True\n",
      "state: -19, action: 1, new state: -9, reward: -5, done?: True\n",
      "state: -9, action: 0, new state: -19, reward: -5, done?: True\n",
      "state: -19, action: 3, new state: -18, reward: -5, done?: True\n",
      "state: -18, action: 2, new state: -19, reward: -5, done?: True\n",
      "state: -19, action: 0, new state: -29, reward: -5, done?: True\n",
      "state: -29, action: 3, new state: -28, reward: -5, done?: True\n",
      "state: -28, action: 3, new state: -27, reward: -5, done?: True\n",
      "state: -27, action: 1, new state: -17, reward: -5, done?: True\n",
      "state: -17, action: 3, new state: -16, reward: -5, done?: True\n",
      "state: -16, action: 0, new state: -26, reward: -5, done?: True\n",
      "state: -26, action: 0, new state: -36, reward: -5, done?: True\n",
      "state: -36, action: 0, new state: -46, reward: -5, done?: True\n",
      "state: -46, action: 3, new state: -45, reward: -5, done?: True\n",
      "state: -45, action: 0, new state: -55, reward: -5, done?: True\n",
      "state: -55, action: 1, new state: -45, reward: -5, done?: True\n",
      "state: -45, action: 1, new state: -35, reward: -5, done?: True\n",
      "state: -35, action: 0, new state: -45, reward: -5, done?: True\n",
      "state: -45, action: 0, new state: -55, reward: -5, done?: True\n",
      "state: -55, action: 2, new state: -56, reward: -5, done?: True\n",
      "state: -56, action: 3, new state: -55, reward: -5, done?: True\n",
      "state: -55, action: 2, new state: -56, reward: -5, done?: True\n",
      "state: -56, action: 3, new state: -55, reward: -5, done?: True\n",
      "state: -55, action: 2, new state: -56, reward: -5, done?: True\n",
      "state: -56, action: 0, new state: -66, reward: -5, done?: True\n",
      "state: -66, action: 0, new state: -76, reward: -5, done?: True\n",
      "state: -76, action: 2, new state: -77, reward: -5, done?: True\n",
      "state: -77, action: 3, new state: -76, reward: -5, done?: True\n",
      "state: -76, action: 0, new state: -86, reward: -5, done?: True\n",
      "state: -86, action: 2, new state: -87, reward: -5, done?: True\n",
      "state: -87, action: 1, new state: -77, reward: -5, done?: True\n",
      "state: -77, action: 3, new state: -76, reward: -5, done?: True\n",
      "state: -76, action: 3, new state: -75, reward: -5, done?: True\n",
      "state: -75, action: 2, new state: -76, reward: -5, done?: True\n",
      "state: -76, action: 0, new state: -86, reward: -5, done?: True\n",
      "state: -86, action: 1, new state: -76, reward: -5, done?: True\n",
      "state: -76, action: 0, new state: -86, reward: -5, done?: True\n",
      "state: -86, action: 0, new state: -96, reward: -5, done?: True\n",
      "state: -96, action: 1, new state: -86, reward: -5, done?: True\n",
      "state: -86, action: 1, new state: -76, reward: -5, done?: True\n",
      "state: -76, action: 0, new state: -86, reward: -5, done?: True\n",
      "state: -86, action: 3, new state: -85, reward: -5, done?: True\n",
      "state: -85, action: 3, new state: -84, reward: -5, done?: True\n",
      "state: -84, action: 0, new state: -94, reward: -5, done?: True\n",
      "state: -94, action: 0, new state: -104, reward: -5, done?: True\n",
      "state: -104, action: 0, new state: -114, reward: -5, done?: True\n",
      "state: -114, action: 3, new state: -113, reward: -5, done?: True\n",
      "state: -113, action: 0, new state: -123, reward: -5, done?: True\n",
      "state: -123, action: 3, new state: -122, reward: -5, done?: True\n",
      "state: -122, action: 0, new state: -132, reward: -5, done?: True\n",
      "state: -132, action: 3, new state: -131, reward: -5, done?: True\n",
      "state: -131, action: 0, new state: -141, reward: -5, done?: True\n",
      "state: -141, action: 2, new state: -142, reward: -5, done?: True\n",
      "state: -142, action: 1, new state: -132, reward: -5, done?: True\n",
      "state: -132, action: 3, new state: -131, reward: -5, done?: True\n",
      "state: -131, action: 0, new state: -141, reward: -5, done?: True\n",
      "state: -141, action: 3, new state: -140, reward: -5, done?: True\n",
      "state: -140, action: 0, new state: -150, reward: -5, done?: True\n",
      "state: -150, action: 0, new state: -160, reward: -5, done?: True\n",
      "state: -160, action: 2, new state: -161, reward: -5, done?: True\n",
      "state: -161, action: 2, new state: -162, reward: -5, done?: True\n",
      "state: -162, action: 0, new state: -172, reward: -5, done?: True\n",
      "state: -172, action: 2, new state: -173, reward: -5, done?: True\n",
      "state: -173, action: 0, new state: -183, reward: -5, done?: True\n",
      "state: -183, action: 0, new state: -193, reward: -5, done?: True\n",
      "state: -193, action: 3, new state: -192, reward: -5, done?: True\n",
      "state: -192, action: 1, new state: -182, reward: -5, done?: True\n",
      "state: -182, action: 2, new state: -183, reward: -5, done?: True\n",
      "state: -183, action: 2, new state: -184, reward: -5, done?: True\n",
      "state: -184, action: 0, new state: -194, reward: -5, done?: True\n",
      "state: -194, action: 0, new state: -204, reward: -5, done?: True\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "index -204 is out of bounds for axis 0 with size 200",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_19396/2130280421.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     29\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     30\u001b[0m                 \u001b[1;31m# Explotation time\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 31\u001b[1;33m                 \u001b[0maction\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mq_table\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mstate\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     32\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     33\u001b[0m             \u001b[1;31m# Take action\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mIndexError\u001b[0m: index -204 is out of bounds for axis 0 with size 200"
     ]
    }
   ],
   "source": [
    "# This cycle is to calculate the average reward/episodes and its only purpose is to plot the nice graph below that\n",
    "# shows how the agent learn how to maximize the reward.\n",
    "for it in range(100):\n",
    "    print('average ', it)\n",
    "    rewards_all_episodes=[]\n",
    "    \n",
    "    # exporation-exploitation trade-off params\n",
    "    exploration_rate = 1\n",
    "    max_exploration_rate = 1\n",
    "    min_exploration_rate = 0.01\n",
    "    exploration_decay_rate = 0.005\n",
    "    \n",
    "    # init q table in zeros\n",
    "    q_table = np.zeros((state_space_size, action_space_size))\n",
    "\n",
    "    # iterate over the episodes\n",
    "    for episode in range(num_episodes):\n",
    "        state = env.reset(initial_map, player_initial_pos)\n",
    "        done = False\n",
    "        rewards_current_episode = 0\n",
    "        \n",
    "        # iterate over the steps for an episode\n",
    "        for step in range(max_steps_per_episode):\n",
    "            # Exploration-exploitation trade-off\n",
    "            exploration_rate_threshold = np.random.uniform(low=0.0, high=1.0)\n",
    "            if exploration_rate_threshold <= exploration_rate:\n",
    "                # Exploration time\n",
    "                action = np.random.randint(0, action_space_size)\n",
    "            else:\n",
    "                # Explotation time\n",
    "                action = np.argmax(q_table[state])\n",
    "\n",
    "            # Take action\n",
    "            new_state, reward, done = env.step(action)\n",
    "\n",
    "            # Update Q-table for Q(s,a)\n",
    "            print(f\"state: {state}, action: {action}, new state: {new_state}, reward: {reward}, done?: {done}\")\n",
    "            if done == True: \n",
    "                break\n",
    "            q_table[state, action] = (1 - learning_rate) * q_table[state, action] + learning_rate * (reward + discount_rate * np.max(q_table[new_state]))\n",
    "            # transition next state\n",
    "\n",
    "            state = new_state\n",
    "            rewards_current_episode += reward\n",
    "            \n",
    "            \n",
    "\n",
    "\n",
    "        # Exploration rate decay\n",
    "        exploration_rate = min_exploration_rate + (max_exploration_rate - min_exploration_rate) * (math.e ** (-exploration_decay_rate * episode))\n",
    "\n",
    "        rewards_all_episodes.append(rewards_current_episode)\n",
    "    rewards_avg.append(rewards_all_episodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = [i for i in range(0,num_episodes)]\n",
    "y = np.mean(rewards_avg, axis=0)\n",
    "plot.xlabel('Episodes')\n",
    "plot.ylabel('Reward')\n",
    "plot.plot(x, y,'o')"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "25c701cf35b356b2d4bd1cf9d31da66e110c57249d3d7829e193c9302586500a"
  },
  "kernelspec": {
   "display_name": "Python 3.8.11 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
